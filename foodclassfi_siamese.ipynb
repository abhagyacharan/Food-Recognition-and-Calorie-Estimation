{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "#from keras.engine import Layer, InputSpec\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import classification_report, log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#/kaggle/input/indian-food-images-dataset/List of Indian Foods.txt\n",
    "#/kaggle/input/indian-food-images-dataset/Indian Food Images/Indian Food Images/mysore_pak/\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    " #   for filename in filenames:\n",
    " #       print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"D:/Github/Datasets/Indian Food Images/Indian Food Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adhirasam',\n",
       " 'aloo_gobi',\n",
       " 'aloo_matar',\n",
       " 'aloo_methi',\n",
       " 'aloo_shimla_mirch',\n",
       " 'aloo_tikki',\n",
       " 'anarsa',\n",
       " 'ariselu',\n",
       " 'bandar_laddu',\n",
       " 'basundi',\n",
       " 'bhatura',\n",
       " 'bhindi_masala',\n",
       " 'biryani',\n",
       " 'boondi',\n",
       " 'butter_chicken',\n",
       " 'chak_hao_kheer',\n",
       " 'cham_cham',\n",
       " 'chana_masala',\n",
       " 'chapati',\n",
       " 'chhena_kheeri',\n",
       " 'chicken_razala',\n",
       " 'chicken_tikka',\n",
       " 'chicken_tikka_masala',\n",
       " 'chikki',\n",
       " 'daal_baati_churma',\n",
       " 'daal_puri',\n",
       " 'dal_makhani',\n",
       " 'dal_tadka',\n",
       " 'dharwad_pedha',\n",
       " 'doodhpak',\n",
       " 'double_ka_meetha',\n",
       " 'dum_aloo',\n",
       " 'gajar_ka_halwa',\n",
       " 'gavvalu',\n",
       " 'ghevar',\n",
       " 'gulab_jamun',\n",
       " 'imarti',\n",
       " 'jalebi',\n",
       " 'kachori',\n",
       " 'kadai_paneer',\n",
       " 'kadhi_pakoda',\n",
       " 'kajjikaya',\n",
       " 'kakinada_khaja',\n",
       " 'kalakand',\n",
       " 'karela_bharta',\n",
       " 'kofta',\n",
       " 'kuzhi_paniyaram',\n",
       " 'lassi',\n",
       " 'ledikeni',\n",
       " 'litti_chokha',\n",
       " 'lyangcha',\n",
       " 'maach_jhol',\n",
       " 'makki_di_roti_sarson_da_saag',\n",
       " 'malapua',\n",
       " 'misi_roti',\n",
       " 'misti_doi',\n",
       " 'modak',\n",
       " 'mysore_pak',\n",
       " 'naan',\n",
       " 'navrattan_korma',\n",
       " 'palak_paneer',\n",
       " 'paneer_butter_masala',\n",
       " 'phirni',\n",
       " 'pithe',\n",
       " 'poha',\n",
       " 'poornalu',\n",
       " 'pootharekulu',\n",
       " 'qubani_ka_meetha',\n",
       " 'rabri',\n",
       " 'rasgulla',\n",
       " 'ras_malai',\n",
       " 'sandesh',\n",
       " 'shankarpali',\n",
       " 'sheera',\n",
       " 'sheer_korma',\n",
       " 'shrikhand',\n",
       " 'sohan_halwa',\n",
       " 'sohan_papdi',\n",
       " 'sutar_feni',\n",
       " 'unni_appam']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_names = os.listdir(train_dir)\n",
    "food_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['adhirasam', 'aloo_gobi', 'aloo_matar', 'aloo_methi',\n",
       "       'aloo_shimla_mirch', 'aloo_tikki', 'anarsa', 'ariselu',\n",
       "       'bandar_laddu', 'basundi', 'bhatura', 'bhindi_masala', 'biryani',\n",
       "       'boondi', 'butter_chicken', 'chak_hao_kheer', 'cham_cham',\n",
       "       'chana_masala', 'chapati', 'chhena_kheeri', 'chicken_razala',\n",
       "       'chicken_tikka', 'chicken_tikka_masala', 'chikki',\n",
       "       'daal_baati_churma', 'daal_puri', 'dal_makhani', 'dal_tadka',\n",
       "       'dharwad_pedha', 'doodhpak', 'double_ka_meetha', 'dum_aloo',\n",
       "       'gajar_ka_halwa', 'gavvalu', 'ghevar', 'gulab_jamun', 'imarti',\n",
       "       'jalebi', 'kachori', 'kadai_paneer', 'kadhi_pakoda', 'kajjikaya',\n",
       "       'kakinada_khaja', 'kalakand', 'karela_bharta', 'kofta',\n",
       "       'kuzhi_paniyaram', 'lassi', 'ledikeni', 'litti_chokha', 'lyangcha',\n",
       "       'maach_jhol', 'makki_di_roti_sarson_da_saag', 'malapua',\n",
       "       'misi_roti', 'misti_doi', 'modak', 'mysore_pak', 'naan',\n",
       "       'navrattan_korma', 'palak_paneer', 'paneer_butter_masala',\n",
       "       'phirni', 'pithe', 'poha', 'poornalu', 'pootharekulu',\n",
       "       'qubani_ka_meetha', 'rabri', 'rasgulla', 'ras_malai', 'sandesh',\n",
       "       'shankarpali', 'sheera', 'sheer_korma', 'shrikhand', 'sohan_halwa',\n",
       "       'sohan_papdi', 'sutar_feni', 'unni_appam'], dtype='<U28')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(food_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(np.array(food_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the 80 classes will be used to train the siamese model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images 1-40 in each class will be used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images 41-50 will be used to validated the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_Shape = (200,200,3)\n",
    "train_ids = np.arange(0,35)\n",
    "val_ids = np.arange(35,45)\n",
    "test_ids = np.arange(45,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(names,n = 0,dataset=\"train\"):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X_train=[]\n",
    "    X_val = []\n",
    "    y_train = []\n",
    "    y_val =[]\n",
    "    X_test = []\n",
    "    y_test =[]\n",
    "    \n",
    "    img_shape = (200,200)\n",
    "    cat_dict = {}\n",
    "    dish_dict = {}\n",
    "    curr_y = n\n",
    "    path = 'D:/Github/Datasets/Indian Food Images/Indian Food Images'\n",
    "    \n",
    "    category_images_tr=[]\n",
    "    category_images_v=[]\n",
    "    category_images_te=[] \n",
    "        \n",
    "    clsctr = 0\n",
    "    for dish in names:\n",
    "        dish_dict[dish] = clsctr\n",
    "        dish_path = os.path.join(path,dish) +'/'\n",
    "        \n",
    "        #category_images_t=[]\n",
    "        #category_images_v=[]\n",
    "                  \n",
    "        #count = 0 \n",
    "        #print(len(os.listdir(dish_path)))\n",
    "        for count, filename in enumerate(os.listdir(dish_path)):\n",
    "            #print(\"Class :{}, count: {}\".format(clsctr,count)) \n",
    "            if count < 35:\n",
    "                #print(\"In train condition\")\n",
    "                image_path = os.path.join(dish_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                resized_image = cv2.resize(image, img_shape)\n",
    "                final_image = resized_image #cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "                y_train.append(clsctr)\n",
    "                try:\n",
    "                    category_images_tr.append(final_image)\n",
    "            \n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    print(\"error - category_images_train:\", final_image)\n",
    "    \n",
    "                \n",
    "            elif count >= 35 and count < 45:\n",
    "                #print(\"In val condition\")\n",
    "                image_path = os.path.join(dish_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                resized_image = cv2.resize(image, img_shape)\n",
    "                final_image = resized_image#cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "                y_val.append(clsctr)\n",
    "                try:\n",
    "                    category_images_v.append(final_image)\n",
    "            \n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    print(\"error - category_images_test:\", final_image)\n",
    "             \n",
    "            \n",
    "            else :\n",
    "                #print(\"In test condition\")\n",
    "                image_path = os.path.join(dish_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                resized_image = cv2.resize(image, img_shape)\n",
    "                final_image = resized_image#cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "                y_test.append(clsctr)\n",
    "                try:\n",
    "                    category_images_te.append(final_image)\n",
    "            \n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    print(\"error - category_images_test:\", final_image)\n",
    "             \n",
    "            curr_y += 1    \n",
    "             \n",
    "             #count = count + 1\n",
    "            \n",
    "        #X_train.append(category_images_t)\n",
    "        #X_val.append(category_images_v)\n",
    "        clsctr += 1\n",
    "    \n",
    "    #y_train = np.vstack(y_train)\n",
    "    #y_val = np.vstack(y_val)\n",
    "    #X_train = np.stack(X_train)\n",
    "    #X_val = np.stack(X_val)\n",
    "    X_train = category_images_tr\n",
    "    X_val = category_images_v\n",
    "    X_test = category_images_te\n",
    "    \n",
    "    #print(y_train)\n",
    "    \n",
    "    return X_train,y_train, X_val,y_val, X_test,y_test, dish_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train,X_val,y_val,X_test,y_test, dish_dict = loadimgs(food_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 800, 2800, 800, 400, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val), len(y_train), len(y_val), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(images, labels):\n",
    "    # initialize two empty lists to hold the (image, image) pairs and\n",
    "    # labels to indicate if a pair is positive or negative\n",
    "    random.seed(2024)\n",
    "    pairImages = []\n",
    "    pairLabels = []\n",
    "   \n",
    "    # calculate the total number of classes present in the dataset\n",
    "    # and then build a list of indexes for each class label that\n",
    "    # provides the indexes for all examples with a given label\n",
    "    numClasses = len(np.unique(y_val))\n",
    "    classes=np.unique(y_val)\n",
    "    idx = [np.where(y_val == classes[i]) for i in range(0, numClasses)]\n",
    "    \n",
    "    # loop over all images\n",
    "    for idxA in range(len(images)):\n",
    "        # grab the current image and label belonging to the current iteration\n",
    "        currentImage = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        \n",
    "        # randomly pick an image that belongs to the *same* class\n",
    "        # label\n",
    "        #posId = random.choice(list(np.where(labels == label)))\n",
    "        posIdx =random.choice([index for index, element in enumerate(labels) if element == label])\n",
    "        posImage = images[posIdx]\n",
    "        \n",
    "        # prepare a positive pair and update the images and labels\n",
    "        pairImages.append([currentImage, posImage])\n",
    "        pairLabels.append([1])\n",
    "        \n",
    "        # grab the indices for each of the class labels *not* equal to\n",
    "        # the current label and randomly pick an image corresponding\n",
    "        # to a label *not* equal to the current label\n",
    "        #negId = random.choice(list(np.where(labels != label)))         \n",
    "        negIdx =random.choice([index for index, element in enumerate(labels) if element != label])\n",
    "        negImage = images[negIdx]\n",
    "        \n",
    "        # prepare a negative pair of images and update our lists\n",
    "        pairImages.append([currentImage, negImage])\n",
    "        pairLabels.append([0])\n",
    "   \n",
    "    return (np.array(pairImages), np.array(pairLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pairTrain, labelTrain) = create_pairs(X_train, y_train)\n",
    "(pairval, labelval) = create_pairs(X_val, y_val)\n",
    "(pairTest, labelTest) = create_pairs(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 200, 3)]     0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 6, 6, 512)         20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32832     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,057,216\n",
      "Trainable params: 2,392,640\n",
      "Non-trainable params: 17,664,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def tf_siamese_nn(shape, embedding=64, fineTune=False):\n",
    "    inputs = tf.keras.layers.Input(shape)\n",
    "    preprocess_fn = preprocess_input\n",
    "    base_model = tf.keras.applications.vgg19.VGG19(input_shape=shape, include_top=False,                                               weights='imagenet')\n",
    "    \n",
    "    if fineTune==False:\n",
    "        base_model.trainable=False\n",
    "    else:\n",
    "        base_model.trainable = True\n",
    "        # Fine-tune from this layer onwards\n",
    "        fine_tune_at = len(base_model.layers)-int(len(base_model.layers)*.10)\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "          layer.trainable =  False\n",
    "    x=base_model(inputs)\n",
    "    x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs=tf.keras.layers.Dense(embedding)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model1=tf_siamese_nn(IMG_Shape, 64, True)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    # unpack the vectors into separate lists\n",
    "    (featsA, featsB) = vectors\n",
    "    # compute the sum of squared distances between the vectors\n",
    "    sumSquared = K.sum(K.square(featsA - featsB), axis=1,keepdims=True)\n",
    "    # return the euclidean distance between the vectors\n",
    "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = tf.keras.layers.Input(shape=IMG_Shape)\n",
    "img2 =  tf.keras.layers.Input( shape=IMG_Shape)\n",
    "featureExtractor = tf_siamese_nn(IMG_Shape)\n",
    "featsA = featureExtractor(img1)\n",
    "featsB = featureExtractor(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "model = tf.keras.Model(inputs=[img1, img2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5600/5600 [==============================] - 292s 51ms/step - loss: 3.1819 - accuracy: 0.5870 - val_loss: 0.6066 - val_accuracy: 0.6775\n",
      "Epoch 2/20\n",
      "5600/5600 [==============================] - 289s 52ms/step - loss: 0.5854 - accuracy: 0.6971 - val_loss: 0.5717 - val_accuracy: 0.6875\n",
      "Epoch 3/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.5492 - accuracy: 0.7316 - val_loss: 0.5647 - val_accuracy: 0.6975\n",
      "Epoch 4/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.5084 - accuracy: 0.7579 - val_loss: 0.5692 - val_accuracy: 0.6931\n",
      "Epoch 5/20\n",
      "5600/5600 [==============================] - 289s 52ms/step - loss: 0.4749 - accuracy: 0.7845 - val_loss: 0.6253 - val_accuracy: 0.6687\n",
      "Epoch 6/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.4466 - accuracy: 0.7989 - val_loss: 0.5905 - val_accuracy: 0.6938\n",
      "Epoch 7/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.4183 - accuracy: 0.8195 - val_loss: 0.5724 - val_accuracy: 0.6988\n",
      "Epoch 8/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.3911 - accuracy: 0.8446 - val_loss: 0.5800 - val_accuracy: 0.7044\n",
      "Epoch 9/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.3726 - accuracy: 0.8536 - val_loss: 0.6254 - val_accuracy: 0.6869\n",
      "Epoch 10/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.3495 - accuracy: 0.8630 - val_loss: 0.5915 - val_accuracy: 0.6981\n",
      "Epoch 11/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.3367 - accuracy: 0.8675 - val_loss: 0.6038 - val_accuracy: 0.7044\n",
      "Epoch 12/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.3167 - accuracy: 0.8788 - val_loss: 0.6275 - val_accuracy: 0.6969\n",
      "Epoch 13/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.3002 - accuracy: 0.8868 - val_loss: 0.6234 - val_accuracy: 0.6938\n",
      "Epoch 14/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.2821 - accuracy: 0.8939 - val_loss: 0.6634 - val_accuracy: 0.6837\n",
      "Epoch 15/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.2653 - accuracy: 0.9079 - val_loss: 0.7235 - val_accuracy: 0.6900\n",
      "Epoch 16/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.2555 - accuracy: 0.9102 - val_loss: 0.6664 - val_accuracy: 0.6981\n",
      "Epoch 17/20\n",
      "5600/5600 [==============================] - 289s 52ms/step - loss: 0.2346 - accuracy: 0.9243 - val_loss: 0.7078 - val_accuracy: 0.6894\n",
      "Epoch 18/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.2254 - accuracy: 0.9275 - val_loss: 1.0267 - val_accuracy: 0.6419\n",
      "Epoch 19/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.2146 - accuracy: 0.9364 - val_loss: 0.7900 - val_accuracy: 0.6725\n",
      "Epoch 20/20\n",
      "5600/5600 [==============================] - 290s 52ms/step - loss: 0.2008 - accuracy: 0.9411 - val_loss: 0.7489 - val_accuracy: 0.6806\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:], validation_data=([pairval[:, 0], pairval[:, 1]], labelval[:]), batch_size=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://e580f7d9-6d51-40d6-9cc2-b42b22413ed0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://e580f7d9-6d51-40d6-9cc2-b42b22413ed0/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"iris_classifier_model.pkl\"  \n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 25s 521ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict( [pairTest[:,0],pairTest[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ = [int(i > .5) for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTest = labelTest.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    # iterate over each label and check\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == predicted:\n",
    "            correct_predictions += 1\n",
    "    # compute the accuracy\n",
    "    accuracy = correct_predictions/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(labelTest,preds_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs_(names):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    images = []\n",
    "    labels = [] \n",
    "    \n",
    "    img_shape = (200,200)\n",
    "    dish_dict = {}\n",
    "    \n",
    "    path = 'D:/Github/Datasets/Indian Food Images/Indian Food Images'\n",
    "    \n",
    "    category_images = []\n",
    "        \n",
    "    clsctr = 0\n",
    "    for dish in names:\n",
    "        dish_dict[dish] = clsctr\n",
    "        dish_path = os.path.join(path,dish) +'/'\n",
    "        \n",
    "        for count, filename in enumerate(os.listdir(dish_path)):\n",
    "            #print(\"Class :{}, count: {}\".format(clsctr,count)) \n",
    "            if count == 1:\n",
    "                #print(\"In train condition\")\n",
    "                image_path = os.path.join(dish_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                resized_image = cv2.resize(image, img_shape)\n",
    "                final_image = resized_image #cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "                labels.append(clsctr)\n",
    "                try:\n",
    "                    category_images.append(final_image)\n",
    "            \n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    print(\"error - category_images_train:\", final_image)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        clsctr += 1\n",
    "        \n",
    "    images = category_images\n",
    "    \n",
    "    return images, labels, dish_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_, labels_ , dish_dict_ = loadimgs_(food_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs_for_pred(image_path, images_):\n",
    "    # initialize two empty lists to hold the (image, image) pairs and\n",
    "    # labels to indicate if a pair is positive or negative\n",
    "    random.seed(2024)\n",
    "    pairImages = []\n",
    "    pairLabels = []\n",
    "   \n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (200,200))\n",
    "                \n",
    "    # loop over all images\n",
    "    for idxA in range(len(images_)):\n",
    "    \n",
    "        currentImage = images_[idxA]\n",
    "\n",
    "        pairImages.append([currentImage, resized_image]) \n",
    "   \n",
    "    return (np.array(pairImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['014ce94539.jpg',\n",
       " '03c77ada4d.jpg',\n",
       " '063a16b50b.jpg',\n",
       " '08cca5782a.jpg',\n",
       " '11a818e3f6.jpg',\n",
       " '12dace47fc.jpg',\n",
       " '15e2514789.jpg',\n",
       " '15f52d7785.jpg',\n",
       " '1b02eccfbe.jpg',\n",
       " '1dbc60a00c.jpg',\n",
       " '1e3f3359d5.jpg',\n",
       " '2ce50a96d1.jpg',\n",
       " '2fff4054b6.jpg',\n",
       " '3a1266de50.jpg',\n",
       " '3a4924c5f0.jpg',\n",
       " '3ac546bb60.jpg',\n",
       " '3b11e111b7.jpg',\n",
       " '3c22b84dd9.jpg',\n",
       " '3d24434b7a.jpg',\n",
       " '3d4802b49c.jpg',\n",
       " '3e474c7fa0.jpg',\n",
       " '3fa1c081db.jpg',\n",
       " '4efcadced4.jpg',\n",
       " '55aa0e6d96.jpg',\n",
       " '55f603e47f.jpg',\n",
       " '5a200c3472.jpg',\n",
       " '5cd440e52d.jpg',\n",
       " '5f675d88e0.jpg',\n",
       " '67a0b5d362.jpg',\n",
       " '69dd42b748.jpg',\n",
       " '6aa91b4eea.jpg',\n",
       " '6ac863dbb4.jpg',\n",
       " '6d71ed5332.jpg',\n",
       " '6d92c18226.jpg',\n",
       " '6e370baada.jpg',\n",
       " '6f58b256d5.jpg',\n",
       " '70b595e1ac.jpg',\n",
       " '72a87c23a2.jpg',\n",
       " '79bc9bded9.jpg',\n",
       " '7c6367c1e6.jpg',\n",
       " '81be6155a9.jpg',\n",
       " '82bf2b6314.jpg',\n",
       " '84b6fcd406.jpg',\n",
       " '86f1c46bd8.jpg',\n",
       " '87d7c05399.jpg',\n",
       " '8ea9cbf7ee.jpg',\n",
       " '9a26b97d4b.jpg',\n",
       " '9c0002e783.jpg',\n",
       " '9d60bf1eb5.jpg',\n",
       " '9edcc97fc8.jpg']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('D:/Github/Datasets/Indian Food Images/Indian Food Images/rasgulla/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path ='D:/Github/Datasets/Indian Food Images/Indian Food Images/rasgulla/55aa0e6d96.jpg'\n",
    "pairPred = create_pairs_for_pred(image_path, images_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 9s 4s/step\n"
     ]
    }
   ],
   "source": [
    "image_prediction = model.predict( [pairPred[:,0],pairPred[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is of rasgulla\n"
     ]
    }
   ],
   "source": [
    "print(\"The image is of {}\".format(list(dish_dict.keys())[list(dish_dict.values()).index(np.argmax(image_prediction))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 537ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image is of chicken_tikka_masala'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(image_path):\n",
    "    pairPred = create_pairs_for_pred(image_path, images_)\n",
    "    image_prediction = model.predict( [pairPred[:,0],pairPred[:,1]])\n",
    "    return \"The image is of {}\".format(list(dish_dict.keys())[list(dish_dict.values()).index(np.argmax(image_prediction))])\n",
    "\n",
    "predict(\"D:/Github/Datasets/Indian Food Images/Indian Food Images/butter_chicken/2ab5e4e96a.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
